{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AKaKETDfNqVEFzv5BnYGSjHfaPtFpH4x","timestamp":1660765340873},{"file_id":"1qFigOOWXcSNyA6hPpZnF-QqhFEB1e-hy","timestamp":1597210805829},{"file_id":"1kGRo0g3UXxXpJuQSEtpKjEGA1Vxbaz8S","timestamp":1597128018290},{"file_id":"1U4oG0A3DFC-XBWhvecYeA3YYReqHpShX","timestamp":1594575042741}],"collapsed_sections":[],"authorship_tag":"ABX9TyP77hFB5yt/KwYfo4xH4Ub0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FT7omJKSM6u1"},"source":["# COURSE: Master calculus 1 using Python: derivatives and applications\n","## SECTION: Applications\n","### LECTURE: CodeChallenge: Gradient descent\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/pycalc_x/?couponCode=202108"]},{"cell_type":"code","source":[],"metadata":{"id":"JpaQU8ZBNATJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JL_0UKJOj1YP"},"source":["# import all necessary modules\n","import numpy as np\n","import sympy as sym\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","display.set_matplotlib_formats('svg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3eOor4_VNAnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OeYMLgEvZY1X"},"source":["# Exercise 1: The function and its derivative"]},{"cell_type":"code","metadata":{"id":"YwTBzVJsoKbg"},"source":["# function (as a function)\n","def fx(x):\n","  return 3*x**2 - 3*x + 4\n","\n","# derivative function\n","def deriv(x):\n","  return 6*x - 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qRE_fHqUL6n"},"source":["# plot the function and its derivative\n","\n","# define a range for x\n","xx = np.linspace(-2,2,2001)\n","\n","# plotting\n","plt.plot(xx,fx(xx), xx,deriv(xx))\n","plt.xlim(xx[[0,-1]])\n","plt.grid()\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.legend(['y','dy'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-f27UUkZYpG"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Gradient descent"],"metadata":{"id":"BBNEJ7-rDrgC"}},{"cell_type":"code","metadata":{"id":"-nzWuHfWVHyU"},"source":["# random starting point\n","localmin = np.random.choice(xx,1)\n","print(f'Started at  x = {localmin[0]:.3f}')\n","\n","# learning parameters\n","learning_rate = .01\n","training_epochs = 100\n","\n","# run through training\n","for i in range(training_epochs):\n","  grad = deriv(localmin)\n","  localmin = localmin - learning_rate*grad\n","\n","print(f'Finished at x = {localmin[0]:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hIord_pVIU8"},"source":["# plot the results\n","\n","plt.plot(xx,fx(xx), xx,deriv(xx))\n","plt.plot(localmin,deriv(localmin),'ro')\n","plt.plot(localmin,fx(localmin),'ro')\n","\n","plt.xlim(xx[[0,-1]])\n","plt.grid()\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.legend(['f(x)','df','f(x) min'])\n","plt.title('Empirical local minimum: %s'%localmin[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ReFLnQn07_-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKN5ul4YVIYb"},"source":["# Exercise 3: Plot the descent"]},{"cell_type":"code","metadata":{"id":"M22aVI6xVIbk"},"source":["# Note: I don't discuss this in the video, \n","# but it's insightful to modify the parameters\n","# and observe the effects on the results. For example,\n","# fix the localmin and adjust the learning rate and epochs!\n","\n","# fix the starting point\n","localmin = -1.5\n","\n","# learning parameters\n","learning_rate = .05\n","training_epochs = 40\n","\n","# run through training and store all the results\n","modelparams = np.zeros((training_epochs,2))\n","for i in range(training_epochs):\n","  grad = deriv(localmin)\n","  localmin = localmin - learning_rate*grad\n","  modelparams[i,0] = localmin\n","  modelparams[i,1] = grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AInqnFtkVIeb"},"source":["# plot the results\n","\n","# plot the function\n","plt.plot(xx,fx(xx), xx,deriv(xx))\n","\n","# plot the progression of points\n","for i,(xi,dx) in enumerate(modelparams):\n","  plt.plot(xi,fx(xi),'o',color=[(i/training_epochs)**(1/2),.2,.2])\n","  plt.plot(xi,deriv(xi),'o',color=[(i/training_epochs)**(1/2)*.7+.3,.1,(i/training_epochs)**(1/2)])\n","\n","\n","# adjust some plotting aspects\n","plt.xlim(xx[[0,-1]])\n","plt.grid()\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.legend(['f(x)','df'])\n","plt.title('Empirical local minimum: %s'%modelparams[-1,0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAGG5fMMVIhM"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Now with sympy"],"metadata":{"id":"1o4Y-JoS8Ap5"}},{"cell_type":"code","source":["x = sym.var('x')\n","\n","# define the function and its derivative\n","fx = sym.sin(x) * (x-2)**2\n","df = sym.diff(fx,x)\n","\n","# quick plot\n","sym.plot(fx,(x,0,5*sym.pi));"],"metadata":{"id":"Rnt3vUHj8Asa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lambdify the functions (easier for plotting)\n","fx_lam = sym.lambdify(x,fx)\n","df_lam = sym.lambdify(x,df)\n","\n","# a nicer-looking plot\n","xx = np.linspace(0,5*np.pi,2001)\n","\n","_,axs = plt.subplots(2,1,figsize=(8,6))\n","axs[0].plot(xx,fx_lam(xx),linewidth=3)\n","axs[0].set_xlim(xx[[0,-1]])\n","axs[0].grid()\n","axs[0].set_title(f'f(x) = ${sym.latex(fx)}$')\n","\n","axs[1].plot(xx,df_lam(xx),linewidth=3)\n","axs[1].plot(xx[[0,-1]],[0,0],'k--')\n","axs[1].set_xlim(xx[[0,-1]])\n","axs[1].grid()\n","axs[1].set_title(f\"f'(x) = ${sym.latex(df)}$\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GZ-SJtIc9A8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YnPSLCpaBFPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Gradient descent in sympy"],"metadata":{"id":"SJNezMZhBFJw"}},{"cell_type":"code","source":["# random starting point in x domain\n","localmin = np.random.choice(xx,1)[0]\n","\n","# learning parameters\n","learning_rate = .05\n","training_epochs = 40\n","\n","\n","# setup the plot with functions\n","plt.figure(figsize=(8,6))\n","plt.plot(xx,fx_lam(xx),label='f(x)')\n","plt.plot(xx,df_lam(xx),label=\"f'(x)\")\n","\n","\n","# run through training and plot immediately\n","for i in range(training_epochs):\n","  \n","  # implement gradient descent\n","  grad = df.subs(x,localmin)\n","  localmin = localmin - learning_rate*grad\n","\n","  # plot the current result\n","  plt.plot(localmin,fx.subs(x,localmin),'o',color=[(i/training_epochs)**(1/2),.2,.2])\n","  plt.plot(localmin,df.subs(x,localmin),'o',color=[(i/training_epochs)**(1/2)*.7+.3,.1,(i/training_epochs)**(1/2)])\n","\n","\n","plt.grid()\n","plt.xlabel('x')\n","plt.ylabel(\"f(x) or f'(x)\")\n","plt.xlim(xx[[0,-1]])\n","plt.legend()\n","plt.title('Empirical local minimum: %s'%localmin)\n","plt.show()"],"metadata":{"id":"ugt7uqohAiRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EgxQNsOqNqpw"},"execution_count":null,"outputs":[]}]}